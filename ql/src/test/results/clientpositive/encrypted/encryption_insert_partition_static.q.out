PREHOOK: query: -- SORT_QUERY_RESULTS

-- init
drop table IF EXISTS encryptedTable PURGE
PREHOOK: type: DROPTABLE
POSTHOOK: query: -- SORT_QUERY_RESULTS

-- init
drop table IF EXISTS encryptedTable PURGE
POSTHOOK: type: DROPTABLE
PREHOOK: query: drop table IF EXISTS unencryptedTable PURGE
PREHOOK: type: DROPTABLE
POSTHOOK: query: drop table IF EXISTS unencryptedTable PURGE
POSTHOOK: type: DROPTABLE
PREHOOK: query: create table encryptedTable(key string,
    value string) partitioned by (ds string) clustered by (key) into 2 buckets stored as orc
#### A masked pattern was here ####
PREHOOK: type: CREATETABLE
#### A masked pattern was here ####
PREHOOK: Output: database:default
PREHOOK: Output: default@encryptedTable
POSTHOOK: query: create table encryptedTable(key string,
    value string) partitioned by (ds string) clustered by (key) into 2 buckets stored as orc
#### A masked pattern was here ####
POSTHOOK: type: CREATETABLE
#### A masked pattern was here ####
POSTHOOK: Output: database:default
POSTHOOK: Output: default@encryptedTable
Encryption key created: 'key_1'
Encryption zone created: '/build/ql/test/data/warehouse/encryptedTable' using key: 'key_1'
PREHOOK: query: create table unencryptedTable(key string,
    value string) partitioned by (ds string) clustered by (key) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true')
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@unencryptedTable
POSTHOOK: query: create table unencryptedTable(key string,
    value string) partitioned by (ds string) clustered by (key) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true')
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@unencryptedTable
PREHOOK: query: -- insert encrypted table from values
explain extended insert into table encryptedTable partition
    (ds='today') values
    ('501', 'val_501'),
    ('502', 'val_502')
PREHOOK: type: QUERY
POSTHOOK: query: -- insert encrypted table from values
explain extended insert into table encryptedTable partition
    (ds='today') values
    ('501', 'val_501'),
    ('502', 'val_502')
POSTHOOK: type: QUERY
ABSTRACT SYNTAX TREE:
  
TOK_QUERY
   TOK_FROM
      null
         null
            Values__Tmp__Table__1
   TOK_INSERT
      TOK_INSERT_INTO
         TOK_TAB
            TOK_TABNAME
               encryptedTable
            TOK_PARTSPEC
               TOK_PARTVAL
                  ds
                  'today'
      TOK_SELECT
         TOK_SELEXPR
            TOK_ALLCOLREF


STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1
  Stage-2 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: values__tmp__table__1
            Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Select Operator
              expressions: tmp_values_col1 (type: string), tmp_values_col2 (type: string)
              outputColumnNames: _col0, _col1
              Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: NONE
              Reduce Output Operator
                sort order: 
                Map-reduce partition columns: _col0 (type: string)
                Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: NONE
                tag: -1
                value expressions: _col0 (type: string), _col1 (type: string)
                auto parallelism: false
      Path -> Alias:
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
      Path -> Partition:
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
          Partition
            base file name: Values__Tmp__Table__1
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              bucket_count -1
              columns tmp_values_col1,tmp_values_col2
              columns.comments 
              columns.types string:string
#### A masked pattern was here ####
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
              name default.values__tmp__table__1
              serialization.ddl struct values__tmp__table__1 { string tmp_values_col1, string tmp_values_col2}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucket_count -1
                columns tmp_values_col1,tmp_values_col2
                columns.comments 
                columns.types string:string
#### A masked pattern was here ####
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
                name default.values__tmp__table__1
                serialization.ddl struct values__tmp__table__1 { string tmp_values_col1, string tmp_values_col2}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.values__tmp__table__1
            name: default.values__tmp__table__1
      Truncated Path -> Alias:
#### A masked pattern was here ####
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: VALUE._col0 (type: string), VALUE._col1 (type: string)
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            GlobalTableId: 1
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/ds=today/.hive-staging
            NumFilesPerFileSink: 1
            Static Partition Specification: ds=today/
            Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: NONE
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/ds=today/.hive-staging
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name key
                  columns key,value
                  columns.comments 
                  columns.types string:string
#### A masked pattern was here ####
                  name default.encryptedtable
                  partition_columns ds
                  partition_columns.types string
                  serialization.ddl struct encryptedtable { string key, string value}
                  serialization.format 1
                  serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                  transactional true
#### A masked pattern was here ####
                serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                name: default.encryptedtable
            TotalFiles: 1
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-0
    Move Operator
      tables:
          partition:
            ds today
          replace: false
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/ds=today/.hive-staging
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name key
                columns key,value
                columns.comments 
                columns.types string:string
#### A masked pattern was here ####
                name default.encryptedtable
                partition_columns ds
                partition_columns.types string
                serialization.ddl struct encryptedtable { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                transactional true
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
              name: default.encryptedtable

  Stage: Stage-2
    Stats-Aggr Operator
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/ds=today/.hive-staging

PREHOOK: query: insert into table encryptedTable partition
    (ds='today') values
    ('501', 'val_501'),
    ('502', 'val_502')
PREHOOK: type: QUERY
PREHOOK: Input: default@values__tmp__table__2
PREHOOK: Output: default@encryptedtable@ds=today
POSTHOOK: query: insert into table encryptedTable partition
    (ds='today') values
    ('501', 'val_501'),
    ('502', 'val_502')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@values__tmp__table__2
POSTHOOK: Output: default@encryptedtable@ds=today
POSTHOOK: Lineage: encryptedtable PARTITION(ds=today).key SIMPLE [(values__tmp__table__2)values__tmp__table__2.FieldSchema(name:tmp_values_col1, type:string, comment:), ]
POSTHOOK: Lineage: encryptedtable PARTITION(ds=today).value SIMPLE [(values__tmp__table__2)values__tmp__table__2.FieldSchema(name:tmp_values_col2, type:string, comment:), ]
PREHOOK: query: select * from encryptedTable order by key
PREHOOK: type: QUERY
PREHOOK: Input: default@encryptedtable
PREHOOK: Input: default@encryptedtable@ds=today
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
POSTHOOK: query: select * from encryptedTable order by key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@encryptedtable
POSTHOOK: Input: default@encryptedtable@ds=today
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
501	val_501	today
502	val_502	today
PREHOOK: query: -- insert encrypted table from unencrypted source
explain extended from src
insert into table encryptedTable partition
    (ds='yesterday')
    select * limit 2
PREHOOK: type: QUERY
POSTHOOK: query: -- insert encrypted table from unencrypted source
explain extended from src
insert into table encryptedTable partition
    (ds='yesterday')
    select * limit 2
POSTHOOK: type: QUERY
ABSTRACT SYNTAX TREE:
  
TOK_QUERY
   TOK_FROM
      TOK_TABREF
         TOK_TABNAME
            src
   TOK_INSERT
      TOK_INSERT_INTO
         TOK_TAB
            TOK_TABNAME
               encryptedTable
            TOK_PARTSPEC
               TOK_PARTVAL
                  ds
                  'yesterday'
      TOK_SELECT
         TOK_SELEXPR
            TOK_ALLCOLREF
      TOK_LIMIT
         2


STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: src
            Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Select Operator
              expressions: key (type: string), value (type: string)
              outputColumnNames: _col0, _col1
              Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
              Limit
                Number of rows: 2
                Statistics: Num rows: 2 Data size: 400 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  sort order: 
                  Statistics: Num rows: 2 Data size: 400 Basic stats: COMPLETE Column stats: NONE
                  tag: -1
                  value expressions: _col0 (type: string), _col1 (type: string)
                  auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: src
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count -1
              columns key,value
              columns.comments 'default','default'
              columns.types string:string
#### A masked pattern was here ####
              name default.src
              numFiles 1
              numRows 0
              rawDataSize 0
              serialization.ddl struct src { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 5812
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE true
                bucket_count -1
                columns key,value
                columns.comments 'default','default'
                columns.types string:string
#### A masked pattern was here ####
                name default.src
                numFiles 1
                numRows 0
                rawDataSize 0
                serialization.ddl struct src { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 5812
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src
            name: default.src
      Truncated Path -> Alias:
        /src [src]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: VALUE._col0 (type: string), VALUE._col1 (type: string)
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 2 Data size: 400 Basic stats: COMPLETE Column stats: NONE
          Limit
            Number of rows: 2
            Statistics: Num rows: 2 Data size: 400 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    columns _col0,_col1
                    columns.types string,string
                    escape.delim \
                    serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              sort order: 
              Map-reduce partition columns: _col0 (type: string)
              Statistics: Num rows: 2 Data size: 400 Basic stats: COMPLETE Column stats: NONE
              tag: -1
              value expressions: _col0 (type: string), _col1 (type: string)
              auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              columns _col0,_col1
              columns.types string,string
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                columns _col0,_col1
                columns.types string,string
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
#### A masked pattern was here ####
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: VALUE._col0 (type: string), VALUE._col1 (type: string)
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 2 Data size: 400 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            GlobalTableId: 1
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/ds=yesterday/.hive-staging
            NumFilesPerFileSink: 1
            Static Partition Specification: ds=yesterday/
            Statistics: Num rows: 2 Data size: 400 Basic stats: COMPLETE Column stats: NONE
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/ds=yesterday/.hive-staging
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name key
                  columns key,value
                  columns.comments 
                  columns.types string:string
#### A masked pattern was here ####
                  name default.encryptedtable
                  partition_columns ds
                  partition_columns.types string
                  serialization.ddl struct encryptedtable { string key, string value}
                  serialization.format 1
                  serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                  transactional true
#### A masked pattern was here ####
                serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                name: default.encryptedtable
            TotalFiles: 1
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-0
    Move Operator
      tables:
          partition:
            ds yesterday
          replace: false
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/ds=yesterday/.hive-staging
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name key
                columns key,value
                columns.comments 
                columns.types string:string
#### A masked pattern was here ####
                name default.encryptedtable
                partition_columns ds
                partition_columns.types string
                serialization.ddl struct encryptedtable { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                transactional true
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
              name: default.encryptedtable

  Stage: Stage-3
    Stats-Aggr Operator
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/ds=yesterday/.hive-staging

PREHOOK: query: from src
insert into table encryptedTable partition
    (ds='yesterday')
    select * limit 2
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@encryptedtable@ds=yesterday
POSTHOOK: query: from src
insert into table encryptedTable partition
    (ds='yesterday')
    select * limit 2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@encryptedtable@ds=yesterday
POSTHOOK: Lineage: encryptedtable PARTITION(ds=yesterday).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: encryptedtable PARTITION(ds=yesterday).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: select * from encryptedTable order by key
PREHOOK: type: QUERY
PREHOOK: Input: default@encryptedtable
PREHOOK: Input: default@encryptedtable@ds=today
PREHOOK: Input: default@encryptedtable@ds=yesterday
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
POSTHOOK: query: select * from encryptedTable order by key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@encryptedtable
POSTHOOK: Input: default@encryptedtable@ds=today
POSTHOOK: Input: default@encryptedtable@ds=yesterday
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
238	val_238	yesterday
501	val_501	today
502	val_502	today
86	val_86	yesterday
PREHOOK: query: -- insert unencrypted table from encrypted source
explain extended from encryptedTable
insert into table unencryptedTable partition
    (ds='today')
    select key, value
PREHOOK: type: QUERY
POSTHOOK: query: -- insert unencrypted table from encrypted source
explain extended from encryptedTable
insert into table unencryptedTable partition
    (ds='today')
    select key, value
POSTHOOK: type: QUERY
ABSTRACT SYNTAX TREE:
  
TOK_QUERY
   TOK_FROM
      TOK_TABREF
         TOK_TABNAME
            encryptedTable
   TOK_INSERT
      TOK_INSERT_INTO
         TOK_TAB
            TOK_TABNAME
               unencryptedTable
            TOK_PARTSPEC
               TOK_PARTVAL
                  ds
                  'today'
      TOK_SELECT
         TOK_SELEXPR
            TOK_TABLE_OR_COL
               key
         TOK_SELEXPR
            TOK_TABLE_OR_COL
               value


STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1
  Stage-2 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: encryptedtable
            Statistics: Num rows: 12 Data size: 2777 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Select Operator
              expressions: key (type: string), value (type: string)
              outputColumnNames: _col0, _col1
              Statistics: Num rows: 12 Data size: 2777 Basic stats: COMPLETE Column stats: NONE
              Reduce Output Operator
                sort order: 
                Map-reduce partition columns: _col0 (type: string)
                Statistics: Num rows: 12 Data size: 2777 Basic stats: COMPLETE Column stats: NONE
                tag: -1
                value expressions: _col0 (type: string), _col1 (type: string)
                auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: ds=today
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              ds today
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count 2
              bucket_field_name key
              columns key,value
              columns.comments 
              columns.types string:string
#### A masked pattern was here ####
              name default.encryptedtable
              numFiles 2
              numRows 0
              partition_columns ds
              partition_columns.types string
              rawDataSize 0
              serialization.ddl struct encryptedtable { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
              totalSize 1392
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name key
                columns key,value
                columns.comments 
                columns.types string:string
#### A masked pattern was here ####
                name default.encryptedtable
                partition_columns ds
                partition_columns.types string
                serialization.ddl struct encryptedtable { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                transactional true
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
              name: default.encryptedtable
            name: default.encryptedtable
#### A masked pattern was here ####
          Partition
            base file name: ds=yesterday
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              ds yesterday
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count 2
              bucket_field_name key
              columns key,value
              columns.comments 
              columns.types string:string
#### A masked pattern was here ####
              name default.encryptedtable
              numFiles 2
              numRows 0
              partition_columns ds
              partition_columns.types string
              rawDataSize 0
              serialization.ddl struct encryptedtable { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
              totalSize 1385
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name key
                columns key,value
                columns.comments 
                columns.types string:string
#### A masked pattern was here ####
                name default.encryptedtable
                partition_columns ds
                partition_columns.types string
                serialization.ddl struct encryptedtable { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                transactional true
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
              name: default.encryptedtable
            name: default.encryptedtable
      Truncated Path -> Alias:
        /encryptedTable/ds=today [encryptedtable]
        /encryptedTable/ds=yesterday [encryptedtable]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: VALUE._col0 (type: string), VALUE._col1 (type: string)
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 12 Data size: 2777 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            GlobalTableId: 1
#### A PARTIAL masked pattern was here #### data/warehouse/unencryptedtable/ds=today/.hive-staging
            NumFilesPerFileSink: 1
            Static Partition Specification: ds=today/
            Statistics: Num rows: 12 Data size: 2777 Basic stats: COMPLETE Column stats: NONE
#### A PARTIAL masked pattern was here #### data/warehouse/unencryptedtable/ds=today/.hive-staging
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name key
                  columns key,value
                  columns.comments 
                  columns.types string:string
#### A masked pattern was here ####
                  name default.unencryptedtable
                  partition_columns ds
                  partition_columns.types string
                  serialization.ddl struct unencryptedtable { string key, string value}
                  serialization.format 1
                  serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                  transactional true
#### A masked pattern was here ####
                serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                name: default.unencryptedtable
            TotalFiles: 1
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-0
    Move Operator
      tables:
          partition:
            ds today
          replace: false
#### A PARTIAL masked pattern was here #### data/warehouse/unencryptedtable/ds=today/.hive-staging
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name key
                columns key,value
                columns.comments 
                columns.types string:string
#### A masked pattern was here ####
                name default.unencryptedtable
                partition_columns ds
                partition_columns.types string
                serialization.ddl struct unencryptedtable { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                transactional true
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
              name: default.unencryptedtable

  Stage: Stage-2
    Stats-Aggr Operator
#### A PARTIAL masked pattern was here #### data/warehouse/unencryptedtable/ds=today/.hive-staging

PREHOOK: query: from encryptedTable
insert into table unencryptedTable partition
    (ds='today')
    select key, value
PREHOOK: type: QUERY
PREHOOK: Input: default@encryptedtable
PREHOOK: Input: default@encryptedtable@ds=today
PREHOOK: Input: default@encryptedtable@ds=yesterday
PREHOOK: Output: default@unencryptedtable@ds=today
POSTHOOK: query: from encryptedTable
insert into table unencryptedTable partition
    (ds='today')
    select key, value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@encryptedtable
POSTHOOK: Input: default@encryptedtable@ds=today
POSTHOOK: Input: default@encryptedtable@ds=yesterday
POSTHOOK: Output: default@unencryptedtable@ds=today
POSTHOOK: Lineage: unencryptedtable PARTITION(ds=today).key SIMPLE [(encryptedtable)encryptedtable.FieldSchema(name:key, type:string, comment:null), ]
POSTHOOK: Lineage: unencryptedtable PARTITION(ds=today).value SIMPLE [(encryptedtable)encryptedtable.FieldSchema(name:value, type:string, comment:null), ]
PREHOOK: query: select * from unencryptedTable order by key
PREHOOK: type: QUERY
PREHOOK: Input: default@unencryptedtable
PREHOOK: Input: default@unencryptedtable@ds=today
#### A masked pattern was here ####
POSTHOOK: query: select * from unencryptedTable order by key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@unencryptedtable
POSTHOOK: Input: default@unencryptedtable@ds=today
#### A masked pattern was here ####
238	val_238	today
501	val_501	today
502	val_502	today
86	val_86	today
PREHOOK: query: -- clean up
drop table encryptedTable PURGE
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@encryptedtable
PREHOOK: Output: default@encryptedtable
POSTHOOK: query: -- clean up
drop table encryptedTable PURGE
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@encryptedtable
POSTHOOK: Output: default@encryptedtable
Encryption key deleted: 'key_1'
PREHOOK: query: drop table unencryptedTable PURGE
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@unencryptedtable
PREHOOK: Output: default@unencryptedtable
POSTHOOK: query: drop table unencryptedTable PURGE
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@unencryptedtable
POSTHOOK: Output: default@unencryptedtable
